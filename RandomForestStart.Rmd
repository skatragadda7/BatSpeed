---
title: "Random Forest Capstone"
output:
  pdf_document: default
  html_document: default
date: "2025-10-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab Report

## Research Question

How does the number of trees influence the performance and stability of predicting the medv of the Boston housing dataset.

## Hypothesis

Initially, an increase in the number of trees will help model stability and prediction. However, there will get to a point where the increase in the number of trees will no longer improve model stability and prediction accuracy. 

## Parameters

The mtry will be fixed at 6. I will test ntree values of 1,5,10,25,50,100,200, and 500. Lastly, I will use test MSE and OOB error to evaluate the models.

## Running the Code

```{r,message=F}
library(MASS)   
library(randomForest)
library(tidyverse)

data1 <- read.csv('onlyCompleteData.csv')

set.seed(123)
train_idx <- which(data1$year == 2024)
train <- data1[train_idx, ]
test <- data1[-train_idx, ]

mtry_val <- 6
ntree_values <- c(1,5,10, 25, 50, 100, 200, 500,1000)
n_reps <- 20

results <- data.frame(ntree=integer(),seed=integer(),test_MSE=double(),OOB_error=double())

for (nt in ntree_values) { # goes through all different ntree values
  for (s in 1:n_reps) { # do many times for each ntree
    set.seed(s)
    rf_model <- randomForest(WRC. ~ ., data=train, mtry=mtry_val, ntree=nt)
    yhat.bag <- predict(rf_model, newdata=test)
    test_mse <- mean((yhat.bag - test$WRC.)^2)
    results <- rbind(results, data.frame(ntree=nt,test_MSE=test_mse,OOB_error=rf_model$mse[nt]))
  }
}

results_all <- results %>% group_by(ntree) %>%summarize(mean_test_MSE = mean(test_MSE),sd_test_MSE = sd(test_MSE),
            mean_OOB_error = mean(OOB_error),sd_OOB_error = sd(OOB_error))
```

```{r}
ggplot(results_all, aes(x=ntree)) + geom_errorbar(aes(ymin=mean_test_MSE-sd_test_MSE, ymax=mean_test_MSE+sd_test_MSE),
                width=20, color="blue") +geom_line(aes(y=mean_test_MSE), color="blue") + geom_point(aes(y=mean_test_MSE), color="blue") +
  geom_line(aes(y=mean_OOB_error), color="red",linetype="dashed") +
  geom_point(aes(y=mean_OOB_error), color="red") +
  labs(title="Effect of ntree on Random Forest Performance",x="Number of Trees (ntree)",y="Error",caption="Blue: Test MSE Â± SD, Red dashed: OOB error") +
  theme_minimal()

results_all
```




```{r}
# Packages
library(MASS)         
library(randomForest)
library(ggplot2)


set.seed(1)
N <- nrow(data1)
train <- which(data1$year == 2024)

boston.train <- data1[train, ]
boston.test  <- data1[-train, ]

# Response vectors
y.test <- boston.test$WRC.

set.seed(1)
bag.boston <- randomForest(WRC. ~ ., data = data1,
                           subset = train, mtry = 12,
                           importance = TRUE)
bag.boston

yhat.bag <- predict(bag.boston, newdata = boston.test)

# Test MSE
mse_bag <- mean((yhat.bag - y.test)^2)
mse_bag

# Plot predicted vs observed
plot(yhat.bag, y.test,
     xlab = "Predicted WRC",
     ylab = "Observed WRC",
     main = "Bagging: Predicted vs Observed (Test)")
abline(0, 1)

set.seed(1)
rf.boston <- randomForest(WRC. ~ ., data = data1,
                          subset = train, mtry = 6,
                          importance = TRUE)

yhat.rf <- predict(rf.boston, newdata = boston.test)
mse_rf <- mean((yhat.rf - y.test)^2)
c(mse_bag = mse_bag, mse_rf = mse_rf)

importance(rf.boston)
varImpPlot(rf.boston, main = "Random Forest Variable Importance")

set.seed(2)
tune.out <- tuneRF(x = boston.train[, setdiff(names(data1), "WRC.")],
                   y = boston.train$WRC.,
                   stepFactor = 1.5,
                   improve = 0.01,
                   ntreeTry = 500,
                   trace = TRUE,
                   plot = TRUE)

# Best mtry from tuneRF (lower OOB error is better)
best.mtry <- tune.out[which.min(tune.out[, "OOBError"]), "mtry"]
best.mtry

set.seed(3)
rf.tuned <- randomForest(WRC. ~ ., data = data1,
                         subset = train, mtry = best.mtry,
                         ntree = 500, importance = TRUE)

mse_rf_tuned <- mean( (predict(rf.tuned, boston.test) - y.test)^2 )
c(mse_bag = mse_bag, mse_rf = mse_rf, mse_rf_tuned = mse_rf_tuned)

# Partial dependence plots give the marginal effect of a feature
par(mfrow = c(1, 2))
partialPlot(rf.tuned, pred.data = boston.test, x.var = "isolated_power",
            main = "PDP: isolated_power")
partialPlot(rf.tuned, pred.data = boston.test, x.var = "avg_swing_speed",
            main = "PDP: avg_swing_speed")
```